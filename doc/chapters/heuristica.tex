\chapter{Heurística}
O modo mais comum para aplicar Inteligência Artificial à jogos é utilizar métodos como \textit{A*} e \textit{MiniMax}, para representar a árvore de busca, em conjunto com uma heurística, desenvolvida com algum conhecimento prévio do jogo. No entanto, desenvolver uma heurística em GGP não é uma tarefa trivial. A maioria dos jogadores GGP tem soluções únicas para esse problema, pois não existe nenhuma pesquisa sólida que diga como gerar uma boa heurística generalista. Algumas abordagem usam características dos jogos, como número de peças, que podem contribuir com a heurística em muitos jogos, mas que em outro conjunto de jogos pode não funcionar ou até mesmo causar prejuízo na avaliação.

Ao lidar com GGP é portanto mais interessante investigar funções de avaliação que não dependem de características específicas, que podem não existir ou fazer sentido em todos jogos. Contar peças, por exemplo, pode ser bom em xadrez mas não faz sentido algum no jogo da velha. A solução adota neste trabalho é uma função de avaliação que explore os estados e descubra através de simulações quão bom ou ruim eles são.


\section{Monte Carlo}
O conjunto de métodos de Monte Carlo\cite{rlearning} necessitam apenas de experiência na interação com o sistema para estimar o valor de uma função de avaliação. A partir da descrição da mecánica do sistema é possível obter a experiência simulando as interações. Um  conjunto de simulações realizadas com um número de passos finitos é conhecida como \textit{tarefa episódica}. Uma única simulação da tarefa é chamada de \textit{episódio} e a pontuação obtida durante o episódio é o \textit{retorno}. 

A ideia fundamental é aprender na média dos retornos obtidos durante as simulações. A experiência é divida em episódios que eventualmente levam a um objetivo e no final deste as estimativas de valores são obtidas. Os métodos de Monte Carlo são, de certa forma, incrementais, pois as médias são alteradas após cada episódio. O termo "Monte Carlo" é frequentemente usado em qualquer estimativa que envolva algum componente randômico. Neste caso, é usada aleatoriedade para percorrer os estados durante as simulações.

Cada estado mantem uma estimativa da recompensa total acumulada, $V(s)$, que o jogador irá atingir caso vá para o estado $s$. Ao final da simulação de cada episódio, o retorno é propagado retroativamente para todos os estados visitados no episódio e a média é recalculada. A média dos retornos pode ser calculada incrementalmente da seguinte maneira:
\begin{equation}
V(s) \leftarrow V(s) + \frac{1}{n(s)}[R - V(s)]
\end{equation}
onde $n(s)$ é o número de visitas ao estado $s$ e $R$ o retorno obtido no términdo da simulação. 

\section{UCT}
Monte Carlo basea-se em repetidas simulações randômicas para gerar resultado. A estratégia mais simples é repetir as simulações até esgotar-se o tempo, e escolher o movimento com melhor retorno. Usar esta estratégia no entanto, faz que o tempo seja gasto igualmente na exploração de bons e de maus movimentos. Se em vez disso fosse usada a informação já obtida, deixando os bons movimentos com um peso maior, o desempenho do jogador seria melhorado, já que usar o tempo explorando quão ruim um mau movimento é, é perda de tempo. 

Este problema é uma variante do \textit{multi armed bandit}\footnote{http://en.wikipedia.org/wiki/Multi-armed\_bandit}, onde uma máquina de caça-níquel tem múltiplas alavancas. Cada alavanca produz uma recompensa aleatória de uma distribuição desconhecida, e a distribuição de recompensas de uma alavanca pode ser diferente das outras. O objetivo é maximar a coleta de recompensa após iterações consecutivas. Puxar alavancas diferentes pode ensinar mais sobre cada uma, porém recompensas maiores podem ser perdidas por usar uma alavanca que não é ótima.

Existem diferentes abordagens para este problema. Uma das estratégias conhecidas é o algoritmo UCB\cite{ucb}. UCB está para Upper Confidence Bounds, onde o algoritmo garante um limite superior (upper bound) do arrependimento causado por não puxar a alavanca ótima. A ideia é que cada alavanca possua um registro da média das recompensas atingidas ao puxa-lá e um viés\footnote{Diferença entre o valor esperado do estimador e o verdadeiro valor do parâmetro a estimar. http://pt.wikipedia.org/wiki/Viés}. O elemento chave desta estratégia é como o víes é calculado. No algoritmo UCB1 ele é dado pela fórmula:
\begin{equation}
\sqrt{\frac{2\ln n}{n_{j}}}
\end{equation}
onde $n_{j}$ é o número de vezes que a alavanca $j$ foi usada e $n$ o total de jogadas feitas. 

Ao aplicar UCB a jogos, o cenário deve ser mudado. No lugar de uma única máquina caça-níquel com alavancas independentes, cada alavanca da primeira máquina vai gerar uma outra máquina ou produzir uma recompensa. Isso corresponde a realizar um movimento no jogo e atingir novo estado ou uma recompensa de um estado final. Ainda sim é possível usar o algoritmo UCB para resolver o problema. O novo método é chamado UCT\cite{uct}, que significa UCB aplicado à árvores (UCB applied to Trees). 

O algoritmo funciona como a estratégia de simualção de Monte Carlo, mas no lugar de escolher ações randômicas ele usa UCB em cada estado para explorar a recompensa que a ação proporcionará. 

A grande vantagem do UCT é que ele não precisa de nenhuma heurística para dar um bom resultado, já que usa recompensas reais para estimar o valor dos movimentos. Também foi provado matematicamente que a probabilidade de escolher uma ação ótima converge para 1 quando o número de simulações cresce. 

Infelizmente existem algumas desvantagens. Se a árvore do jogo for muito grande ou cada mudança de estado for computacionamente complexa para ser calculada, o algoritmo pode apenas algumas vezes ou nunca atingir um estado terminal, deixando a tomada de decisão infundada. Há também o caso em que um movimento inicialmente parace bom mas na verdade é mau, podendo enganar o algoritmo se ele não tiver tempo o suficiente para simular e consertar o erro. 